# kNN算法介绍
导语：商业哲学家 Jim Rohn 说过一句话，“你，就是你最常接触的五个人的平均。”那么，在分析一个人时，我们不妨观察和他最亲密的几个人。同理的，在判定一个未知事物时，可以观察离它最近的几个样本，这就是 kNN（k最近邻）的方法。

kNN算法可以用来进行分类和回归。

k近邻算法就是说给定一个数据集，和一个输入实例，在数据集中找到与输入实例最近的k个实例，这个k个实例属于哪个类别，就判断输入实例也属于此类别。

选择不同的k值，会影响最终的结果，如何确定k值？

如果选取了较小的k值，就意味着模型变得复杂，容易发生过拟合
这里模型复杂是指过于精密，很容易学习到噪声。
过拟合的含义是指过于紧密或精确地匹配特定数据集，以致于无法良好地拟合其他数据或预测未来的观察结果的现象。

k值的增大会导致模型变得简单，比方说极端情况k=N（训练样本的个数），就会导致无论输入什么，都将预测它为训练实例中最多的类别。这种情况，相当于模型压根没得到训练。

通常采用 交叉验证法 来选取最优的k值。这里不做深入。

距离的度量一般采用欧氏距离。

特征归一化，不同特征他们的差值比例关系失衡，为了保证每个特征同等重要，需要归一化

# 再看看kNN算法的具体实现：KD树
kd树是对数据点在N维空间中划分的一种数据结构，主要用于多维空间关键数据的搜索（如范围搜索和最近邻搜索）。
如果特征的维度是D，样本的数量是N，那么一般来讲 kd 树算法的复杂度是 O(DlogN)，相比于穷举的 O(DN) 省去了非常多的计算量。

本质上说，kd树就是一种平衡二叉树。它的每一个节点记载了【特征坐标、切分轴、指向左枝和右枝的指针】
特征坐标是指对空间进行切分的点的坐标。
切分轴是指从第几维进行的分割。
节点的左枝和右枝分别都是kd数，并且满足：如果y是左枝的



**引申出如何查找中位数。**

更详细的参考：
https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272
https://www.joinquant.com/view/community/detail/c2c41c79657cebf8cd871b44ce4f5d97
